"""
Smart Ingestion Gateway - FastAPI Backend
Ø¨ÙˆØ§Ø¨Ø© Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø°ÙƒÙŠØ© - Ø§Ù„Ø®Ø§Ø¯Ù… Ø§Ù„Ø®Ù„ÙÙŠ

Ù‡Ø°Ø§ Ø§Ù„Ù†Ø¸Ø§Ù… ÙŠÙˆÙØ±:
1. ØªØ­Ù„ÙŠÙ„ Ø°ÙƒÙŠ Ù„Ù„Ù…Ù„ÙØ§Øª (Excel/CSV)
2. ÙƒØ´Ù ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¹Ù† ØµÙ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†
3. ØªØµÙ†ÙŠÙ Ø¯Ù„Ø§Ù„ÙŠ Ù„Ù„Ø£Ø¹Ù…Ø¯Ø©
4. Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
"""

from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
import pandas as pd
import openpyxl
import io
import re
from datetime import datetime
import tempfile
import os
from pathlib import Path

app = FastAPI(
    title="Smart Ingestion Gateway",
    description="Ø¨ÙˆØ§Ø¨Ø© Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø°ÙƒÙŠØ© Ù„Ù…Ù†ØµØ© Ø¥Ø¨Ø¯Ø§Ø¹",
    version="1.0.0"
)

# CORS Configuration - Ø§Ù„Ø³Ù…Ø§Ø­ Ù„Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø£Ù…Ø§Ù…ÙŠØ© Ø¨Ø§Ù„Ø§ØªØµØ§Ù„
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ÙÙŠ Ø§Ù„Ø¥Ù†ØªØ§Ø¬: Ø­Ø¯Ø¯ Ø§Ù„Ù†Ø·Ø§Ù‚Ø§Øª Ø§Ù„Ù…Ø³Ù…ÙˆØ­Ø©
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============================================
# Models - Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
# ============================================

class ColumnInfo(BaseModel):
    index: int
    header: str
    type: str
    semantic_guess: str
    confidence: float
    sample_values: List[Any]

class AnalyzeResponse(BaseModel):
    success: bool
    detected_header_row: int
    total_rows: int
    total_data_rows: int
    columns: List[ColumnInfo]
    preview_rows: List[List[Any]]
    file_id: str

class MappingRule(BaseModel):
    source_column: str  # Ø§Ø³Ù… Ø§Ù„Ø¹Ù…ÙˆØ¯ ÙÙŠ Ø§Ù„Ù…Ù„Ù
    target_field: str   # Ø§Ø³Ù… Ø§Ù„Ø­Ù‚Ù„ ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù…

class ProcessRequest(BaseModel):
    file_id: str
    mapping: List[MappingRule]
    skip_empty: bool = True

class ProcessedRecord(BaseModel):
    data: Dict[str, Any]
    warnings: List[str] = []

class ProcessResponse(BaseModel):
    success: bool
    processed_data: List[ProcessedRecord]
    total_processed: int
    total_skipped: int
    report: str

# ============================================
# Helper Functions - Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø©
# ============================================

# ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚Øª Ù„Ù„Ù…Ù„ÙØ§Øª (ÙÙŠ Ø§Ù„Ø¥Ù†ØªØ§Ø¬ Ø§Ø³ØªØ®Ø¯Ù… Redis Ø£Ùˆ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª)
TEMP_FILES = {}

def detect_arabic_keywords(text: str) -> Dict[str, float]:
    """
    ÙƒØ´Ù Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ¥Ø±Ø¬Ø§Ø¹ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø«Ù‚Ø©
    """
    if not isinstance(text, str):
        return {}
    
    text_lower = text.lower().strip()
    
    patterns = {
        'student_name': [
            'Ø§Ø³Ù…', 'Ø§Ù„Ø§Ø³Ù…', 'Ø·Ø§Ù„Ø¨', 'Ù…ØªØ¯Ø±Ø¨', 'name', 'student'
        ],
        'student_email': [
            'Ø¨Ø±ÙŠØ¯', 'Ø§ÙŠÙ…ÙŠÙ„', 'email', 'mail', '@'
        ],
        'student_phone': [
            'Ù‡Ø§ØªÙ', 'Ø¬ÙˆØ§Ù„', 'phone', 'mobile', 'tel'
        ],
        'course_title': [
            'Ø¯ÙˆØ±Ø©', 'ÙƒÙˆØ±Ø³', 'Ø¨Ø±Ù†Ø§Ù…Ø¬', 'course', 'program'
        ],
        'grade_value': [
            'Ø¯Ø±Ø¬Ø©', 'Ù†ØªÙŠØ¬Ø©', 'Ø¹Ù„Ø§Ù…Ø©', 'grade', 'score', 'mark'
        ],
        'grade_percent': [
            'Ù†Ø³Ø¨Ø©', 'Ù…Ø¦ÙˆÙŠØ©', 'percent', '%'
        ],
        'governorate': [
            'Ù…Ø­Ø§ÙØ¸Ø©', 'Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø©', 'governorate', 'province'
        ],
        'district': [
            'Ù…Ø¯ÙŠØ±ÙŠØ©', 'Ù…Ù†Ø·Ù‚Ø©', 'district', 'region'
        ],
        'date': [
            'ØªØ§Ø±ÙŠØ®', 'date', 'ÙˆÙ‚Øª', 'time'
        ],
        'status': [
            'Ø­Ø§Ù„Ø©', 'Ø§Ù„Ø­Ø§Ù„Ø©', 'status', 'state'
        ],
        'notes': [
            'Ù…Ù„Ø§Ø­Ø¸Ø§Øª', 'Ù…Ù„Ø§Ø­Ø¸Ø©', 'notes', 'note', 'ØªØ¹Ù„ÙŠÙ‚'
        ]
    }
    
    matches = {}
    for semantic_type, keywords in patterns.items():
        for keyword in keywords:
            if keyword in text_lower:
                # Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø«Ù‚Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…Ø¯Ù‰ ØªØ·Ø§Ø¨Ù‚ Ø§Ù„ÙƒÙ„Ù…Ø©
                if text_lower == keyword:
                    confidence = 0.95
                elif text_lower.startswith(keyword) or text_lower.endswith(keyword):
                    confidence = 0.85
                else:
                    confidence = 0.70
                
                if semantic_type not in matches or confidence > matches[semantic_type]:
                    matches[semantic_type] = confidence
    
    return matches

def infer_data_type(series: pd.Series) -> str:
    """
    Ø§Ø³ØªÙ†ØªØ§Ø¬ Ù†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„Ø¹ÙŠÙ†Ø©
    """
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„ÙØ§Ø±ØºØ©
    series_clean = series.dropna()
    if len(series_clean) == 0:
        return "empty"
    
    # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ø±Ù‚Ù…
    try:
        pd.to_numeric(series_clean)
        return "numeric"
    except:
        pass
    
    # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ ØªØ§Ø±ÙŠØ®
    try:
        pd.to_datetime(series_clean, errors='raise')
        return "datetime"
    except:
        pass
    
    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ
    email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}'
    if series_clean.astype(str).str.match(email_pattern).sum() > len(series_clean) * 0.5:
        return "email"
    
    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù‡Ø§ØªÙ
    phone_pattern = r'[\d\s\-\+\(\)]{8,}'
    if series_clean.astype(str).str.match(phone_pattern).sum() > len(series_clean) * 0.5:
        return "phone"
    
    return "string"

def detect_header_row(df: pd.DataFrame, max_search_rows: int = 50) -> int:
    """
    ÙƒØ´Ù Ø°ÙƒÙŠ Ø¹Ù† ØµÙ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†
    
    Ø§Ù„Ø·Ø±ÙŠÙ‚Ø©:
    1. Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø£ÙˆÙ„ 50 ØµÙ
    2. ØªØ­Ù„ÙŠÙ„ ÙƒÙ„ ØµÙ ÙˆØ­Ø³Ø§Ø¨ "Ø¯Ø±Ø¬Ø© Ø§Ù„Ø¹Ù†ÙˆØ§Ù†"
    3. Ø§Ù„ØµÙ Ø°Ùˆ Ø£Ø¹Ù„Ù‰ Ø¯Ø±Ø¬Ø© Ù‡Ùˆ ØµÙ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†
    """
    scores = []
    
    for idx in range(min(max_search_rows, len(df))):
        row = df.iloc[idx]
        score = 0
        
        # Ø¹Ø¯Ø¯ Ø§Ù„Ø®Ù„Ø§ÙŠØ§ ØºÙŠØ± Ø§Ù„ÙØ§Ø±ØºØ©
        non_empty = row.notna().sum()
        if non_empty < len(row) * 0.5:  # Ø£Ù‚Ù„ Ù…Ù† 50% Ù…Ù…ØªÙ„Ø¦
            scores.append(0)
            continue
        
        score += non_empty * 2
        
        # Ø·ÙˆÙ„ Ø§Ù„Ù†Øµ (Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† Ø¹Ø§Ø¯Ø© Ù‚ØµÙŠØ±Ø©)
        text_lengths = [len(str(val)) for val in row if pd.notna(val)]
        if text_lengths:
            avg_length = sum(text_lengths) / len(text_lengths)
            if 3 <= avg_length <= 50:
                score += 10
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ©
        for val in row:
            if pd.notna(val):
                keywords = detect_arabic_keywords(str(val))
                if keywords:
                    score += 15
        
        # ØªÙØ¶ÙŠÙ„ Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ø£ÙˆÙ„Ù‰ Ù‚Ù„ÙŠÙ„Ø§Ù‹
        if idx < 5:
            score += 3
        
        scores.append(score)
    
    if not scores or max(scores) == 0:
        return 0
    
    return scores.index(max(scores))

def analyze_column_semantics(header: str, data_series: pd.Series, data_type: str) -> tuple:
    """
    ØªØ­Ù„ÙŠÙ„ Ø¯Ù„Ø§Ù„ÙŠ Ù„Ù„Ø¹Ù…ÙˆØ¯ ÙˆØ¥Ø±Ø¬Ø§Ø¹ (Ø§Ù„ØªØµÙ†ÙŠÙØŒ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø«Ù‚Ø©)
    """
    # Ø£ÙˆÙ„Ø§Ù‹: Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
    keyword_matches = detect_arabic_keywords(header)
    
    if keyword_matches:
        best_match = max(keyword_matches.items(), key=lambda x: x[1])
        semantic_type, confidence = best_match
        
        # ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø«Ù‚Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ØªÙˆØ§ÙÙ‚ Ù†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        if semantic_type == 'grade_value' and data_type != 'numeric':
            confidence *= 0.7
        elif semantic_type == 'student_email' and data_type != 'email':
            confidence *= 0.7
        elif semantic_type == 'student_phone' and data_type != 'phone':
            confidence *= 0.7
        
        return semantic_type, confidence
    
    # Ø«Ø§Ù†ÙŠØ§Ù‹: Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    if data_type == 'numeric':
        # Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø¯Ø±Ø¬Ø© Ø£Ùˆ Ø¹Ù…Ø± Ø£Ùˆ Ø±Ù‚Ù…
        return 'numeric_field', 0.50
    elif data_type == 'email':
        return 'student_email', 0.85
    elif data_type == 'phone':
        return 'student_phone', 0.85
    elif data_type == 'datetime':
        return 'date_field', 0.70
    
    return 'text_field', 0.30

# ============================================
# API Endpoints - Ù†Ù‚Ø§Ø· Ø§Ù„Ù†Ù‡Ø§ÙŠØ©
# ============================================

@app.get("/")
async def root():
    """Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    return {
        "service": "Smart Ingestion Gateway",
        "version": "1.0.0",
        "status": "running",
        "endpoints": {
            "analyze": "POST /analyze_spreadsheet",
            "process": "POST /process_spreadsheet"
        }
    }

@app.post("/analyze_spreadsheet", response_model=AnalyzeResponse)
async def analyze_spreadsheet(file: UploadFile = File(...)):
    """
    ØªØ­Ù„ÙŠÙ„ Ù…Ù„Ù Excel/CSV ÙˆØ§ÙƒØªØ´Ø§Ù Ø§Ù„Ø¨Ù†ÙŠØ©
    
    - ÙŠÙƒØªØ´Ù ØµÙ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹
    - ÙŠØµÙ†Ù Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø¯Ù„Ø§Ù„ÙŠØ§Ù‹
    - ÙŠØ¹ÙŠØ¯ Ù…Ø¹Ø§ÙŠÙ†Ø© Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    """
    try:
        # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù
        contents = await file.read()
        file_extension = Path(file.filename).suffix.lower()
        
        # ØªØ­Ø¯ÙŠØ¯ Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù
        if file_extension in ['.xlsx', '.xls']:
            if file_extension == '.xlsx':
                df = pd.read_excel(io.BytesIO(contents), engine='openpyxl', header=None)
            else:
                df = pd.read_excel(io.BytesIO(contents), engine='xlrd', header=None)
        elif file_extension == '.csv':
            df = pd.read_csv(io.BytesIO(contents), header=None, encoding='utf-8-sig')
        else:
            raise HTTPException(status_code=400, detail="Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…. Ø§Ø³ØªØ®Ø¯Ù… Excel Ø£Ùˆ CSV")
        
        if df.empty:
            raise HTTPException(status_code=400, detail="Ø§Ù„Ù…Ù„Ù ÙØ§Ø±Øº")
        
        # ÙƒØ´Ù ØµÙ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†
        header_row_idx = detect_header_row(df)
        
        # Ø¥Ø¹Ø§Ø¯Ø© Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ ØªØ­Ø¯ÙŠØ¯ ØµÙ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†
        if file_extension in ['.xlsx', '.xls']:
            if file_extension == '.xlsx':
                df = pd.read_excel(io.BytesIO(contents), engine='openpyxl', header=header_row_idx)
            else:
                df = pd.read_excel(io.BytesIO(contents), engine='xlrd', header=header_row_idx)
        else:
            df = pd.read_csv(io.BytesIO(contents), header=header_row_idx, encoding='utf-8-sig')
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
        columns_info = []
        for idx, col_name in enumerate(df.columns):
            col_data = df[col_name]
            
            # Ø§Ø³ØªÙ†ØªØ§Ø¬ Ù†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            data_type = infer_data_type(col_data)
            
            # Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ
            semantic_type, confidence = analyze_column_semantics(
                str(col_name), 
                col_data, 
                data_type
            )
            
            # Ø£Ø®Ø° Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ù‚ÙŠÙ…
            sample_values = col_data.head(3).fillna("").tolist()
            
            columns_info.append(ColumnInfo(
                index=idx,
                header=str(col_name),
                type=data_type,
                semantic_guess=semantic_type,
                confidence=round(confidence, 2),
                sample_values=sample_values
            ))
        
        # Ù…Ø¹Ø§ÙŠÙ†Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ø£ÙˆÙ„ 5 ØµÙÙˆÙ)
        preview_rows = df.head(5).fillna("").values.tolist()
        
        # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ù…Ø¤Ù‚ØªØ§Ù‹ Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„Ø§Ø­Ù‚Ø©
        file_id = f"file_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}"
        TEMP_FILES[file_id] = {
            'content': contents,
            'filename': file.filename,
            'header_row': header_row_idx,
            'uploaded_at': datetime.now()
        }
        
        return AnalyzeResponse(
            success=True,
            detected_header_row=header_row_idx,
            total_rows=len(df) + header_row_idx + 1,
            total_data_rows=len(df),
            columns=columns_info,
            preview_rows=preview_rows,
            file_id=file_id
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ÙØ´Ù„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù„Ù: {str(e)}")

@app.post("/process_spreadsheet", response_model=ProcessResponse)
async def process_spreadsheet(request: ProcessRequest):
    """
    Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù ÙˆØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø±Ø¨Ø· (Mapping)
    
    - ÙŠØ·Ø¨Ù‚ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø±Ø¨Ø· Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©
    - ÙŠÙ†Ø¸Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    - ÙŠØ¹ÙŠØ¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¬Ø§Ù‡Ø²Ø© Ù„Ù„Ø¥Ø¯Ø®Ø§Ù„
    """
    try:
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ù„Ù
        if request.file_id not in TEMP_FILES:
            raise HTTPException(status_code=404, detail="Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ Ø£Ùˆ Ø§Ù†ØªÙ‡Øª ØµÙ„Ø§Ø­ÙŠØªÙ‡")
        
        file_info = TEMP_FILES[request.file_id]
        contents = file_info['content']
        header_row = file_info['header_row']
        
        # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù
        file_extension = Path(file_info['filename']).suffix.lower()
        if file_extension in ['.xlsx', '.xls']:
            if file_extension == '.xlsx':
                df = pd.read_excel(io.BytesIO(contents), engine='openpyxl', header=header_row)
            else:
                df = pd.read_excel(io.BytesIO(contents), engine='xlrd', header=header_row)
        else:
            df = pd.read_excel(io.BytesIO(contents), header=header_row, encoding='utf-8-sig')
        
        # Ø¨Ù†Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø±Ø¨Ø·
        mapping_dict = {rule.source_column: rule.target_field for rule in request.mapping}
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        processed_records = []
        skipped_count = 0
        
        for idx, row in df.iterrows():
            # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø±Ø¨Ø·
            mapped_data = {}
            warnings = []
            is_empty = True
            
            for source_col, target_field in mapping_dict.items():
                if source_col not in df.columns:
                    warnings.append(f"Ø§Ù„Ø¹Ù…ÙˆØ¯ '{source_col}' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
                    continue
                
                value = row[source_col]
                
                # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„ÙØ§Ø±ØºØ©
                if pd.isna(value) or value == "":
                    if request.skip_empty:
                        continue
                    value = None
                else:
                    is_empty = False
                    
                    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†ÙˆØ¹ Ø§Ù„Ø­Ù‚Ù„
                    if 'grade' in target_field.lower() or 'percent' in target_field.lower():
                        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ø±Ù‚Ù…
                        try:
                            value = float(str(value).replace('%', '').replace(',', '.').strip())
                        except:
                            warnings.append(f"ÙØ´Ù„ ØªØ­ÙˆÙŠÙ„ '{value}' Ø¥Ù„Ù‰ Ø±Ù‚Ù… ÙÙŠ {target_field}")
                            value = None
                    
                    elif 'email' in target_field.lower():
                        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ
                        value = str(value).strip().lower()
                        if '@' not in value:
                            warnings.append(f"Ø¨Ø±ÙŠØ¯ Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ ØºÙŠØ± ØµØ§Ù„Ø­: {value}")
                    
                    elif 'phone' in target_field.lower():
                        # ØªÙ†Ø¸ÙŠÙ Ø±Ù‚Ù… Ø§Ù„Ù‡Ø§ØªÙ
                        value = str(value).strip()
                        value = re.sub(r'[^\d+]', '', value)
                    
                    else:
                        # Ù†Øµ Ø¹Ø§Ø¯ÙŠ
                        value = str(value).strip()
                
                mapped_data[target_field] = value
            
            # ØªØ®Ø·ÙŠ Ø§Ù„ØµÙÙˆÙ Ø§Ù„ÙØ§Ø±ØºØ©
            if is_empty and request.skip_empty:
                skipped_count += 1
                continue
            
            processed_records.append(ProcessedRecord(
                data=mapped_data,
                warnings=warnings
            ))
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        report = f"""
ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù Ø¨Ù†Ø¬Ø§Ø­!
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“Š Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {len(processed_records)}
â­ï¸  Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ù…ØªØ®Ø·Ø§Ø© (ÙØ§Ø±ØºØ©): {skipped_count}
âœ… Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ø¬Ø§Ù‡Ø²Ø© Ù„Ù„Ø¥Ø¯Ø®Ø§Ù„: {len(processed_records)}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        """.strip()
        
        return ProcessResponse(
            success=True,
            processed_data=processed_records,
            total_processed=len(processed_records),
            total_skipped=skipped_count,
            report=report
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ÙØ´Ù„Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {str(e)}")

@app.delete("/cleanup/{file_id}")
async def cleanup_file(file_id: str):
    """
    ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø¤Ù‚ØªØ©
    """
    if file_id in TEMP_FILES:
        del TEMP_FILES[file_id]
        return {"success": True, "message": "ØªÙ… Ø­Ø°Ù Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¤Ù‚Øª"}
    return {"success": False, "message": "Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯"}

# ============================================
# Startup
# ============================================

if __name__ == "__main__":
    import uvicorn
    print("ğŸš€ Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø¨ÙˆØ§Ø¨Ø© Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø°ÙƒÙŠØ©...")
    print("ğŸ“ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†: http://localhost:8008")
    print("ğŸ“– Ø§Ù„ØªÙˆØ«ÙŠÙ‚: http://localhost:8008/docs")
    uvicorn.run(app, host="0.0.0.0", port=8008)
